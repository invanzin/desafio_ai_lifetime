"""
M√≥dulo de reparo de JSON malformado usando LLM.

Este m√≥dulo cont√©m a l√≥gica de reparo autom√°tico de JSONs que falharam
na valida√ß√£o Pydantic. √â usado tanto pelo Extractor quanto pelo Analyzer.

Caracter√≠sticas:
- Parametriz√°vel por tipo de schema (extract/analyze)
- Timeout configur√°vel (15s padr√£o)
- LangSmith tracing integrado
- Logging estruturado
"""

import json
import asyncio
import logging
from typing import Literal

# LangChain imports
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser

# Schemas internos
from app.models.schemas_common import NormalizedInput

# LLM client
from llm.openai_client import default_client

# Utilit√°rios compartilhados
from utils.common import sanitize_transcript_for_log


# ============================================================================
# CONFIGURA√á√ÉO
# ============================================================================

# Logger (configurado centralmente via app.config.logging_config)
logger = logging.getLogger(__name__)

# Timeout para opera√ß√£o de reparo (menor que LLM principal)
REPAIR_TIMEOUT_SECONDS = 15

# LLM e Parser
llm = default_client.get_llm()
parser = JsonOutputParser()


# ============================================================================
# SCHEMAS ESPERADOS POR TIPO
# ============================================================================

EXTRACTOR_SCHEMA = """
- meeting_id, customer_id, customer_name (strings obrigat√≥rias)
- banker_id, banker_name (strings obrigat√≥rias)
- meet_type (string obrigat√≥ria)
- meet_date (datetime ISO 8601 obrigat√≥rio)
- summary (string com 100-200 palavras EXATAS)
- key_points (array de strings)
- action_items (array de strings)
- topics (array de strings)
- source: "lftm-challenge"
- idempotency_key: ser√° preenchido externamente
- transcript_ref: null
- duration_sec: null
"""

ANALYZER_SCHEMA = """
- meeting_id, customer_id, customer_name (strings obrigat√≥rias)
- banker_id, banker_name (strings obrigat√≥rias)
- meet_type (string obrigat√≥ria)
- meet_date (datetime ISO 8601 obrigat√≥rio)
- sentiment_label: "positive"/"neutral"/"negative" (string obrigat√≥ria)
- sentiment_score: float entre 0.0 e 1.0 (obrigat√≥rio)
- summary (string com 100-200 palavras EXATAS)
- key_points (array de strings)
- action_items (array de strings)
- risks (array de strings, pode ser vazio)
- source: "lftm-challenge"
- idempotency_key: ser√° preenchido externamente
"""


# ============================================================================
# FUN√á√ÉO DE REPARO
# ============================================================================

async def repair_json(
    malformed_output: dict,
    validation_error: str,
    normalized: NormalizedInput,
    request_id: str,
    schema_type: Literal["extract", "analyze"] = "extract"
) -> dict:
    """
    Tenta reparar um JSON malformado reenviando ao LLM com o erro.
    
    Esta fun√ß√£o √© chamada quando a valida√ß√£o Pydantic falha na primeira tentativa.
    Envia o JSON original + mensagem de erro de volta ao LLM, pedindo corre√ß√£o.
    
    A fun√ß√£o √© parametrizada por `schema_type` para suportar diferentes schemas:
    - "extract": Para ExtractedMeeting (campos: topics, etc.)
    - "analyze": Para AnalyzedMeeting (campos: sentiment_label, risks, etc.)
    
    Args:
        malformed_output: O JSON original que falhou na valida√ß√£o
        validation_error: Mensagem de erro da valida√ß√£o Pydantic
        normalized: Dados normalizados da reuni√£o (para contexto)
        request_id: ID de correla√ß√£o da requisi√ß√£o
        schema_type: Tipo de schema a ser reparado ("extract" ou "analyze")
    
    Returns:
        dict: JSON reparado (esperan√ßosamente v√°lido)
    
    Raises:
        asyncio.TimeoutError: Se a opera√ß√£o demorar mais de REPAIR_TIMEOUT_SECONDS
        Exception: Se a chamada ao LLM falhar
    
    Example:
        >>> repaired = await repair_json(
        ...     malformed_output={"meeting_id": "MTG123", "summary": "texto curto"},
        ...     validation_error="summary deve ter 100-200 palavras",
        ...     normalized=normalized_input,
        ...     request_id="req-123",
        ...     schema_type="extract"
        ... )
    """
    logger.warning(
        f"[REPAIR] [{request_id}] üîß Tentando reparar JSON inv√°lido | "
        f"schema_type={schema_type} | "
        f"erro={validation_error[:200]}"
    )
    
    # Seleciona o schema esperado baseado no tipo
    expected_schema = EXTRACTOR_SCHEMA if schema_type == "extract" else ANALYZER_SCHEMA
    
    # Cria o prompt de reparo
    repair_prompt = ChatPromptTemplate.from_messages([
        ("system", """Voc√™ √© um corretor de JSON especializado.
Corrija o JSON abaixo para atender ao schema especificado.
Mantenha o m√°ximo de informa√ß√µes corretas poss√≠vel.
Responda APENAS com o JSON corrigido, sem explica√ß√µes."""),
                
        ("human", """JSON MALFORMADO:
{malformed_json}

ERRO DE VALIDA√á√ÉO:
{error}

TRANSCRI√á√ÉO ORIGINAL (para refer√™ncia se precisar):
{transcript_preview}

SCHEMA ESPERADO:
{expected_schema}

Retorne o JSON corrigido:""")
    ])
    
    # Cria a chain de reparo
    repair_chain = repair_prompt | llm | parser
    
    try:
        # Configura√ß√£o para o trace do reparo no LangSmith
        repair_trace_config = {
            "metadata": {
                "request_id": request_id,
                "schema_type": schema_type,
                "operation": "repair_json"
            },
            "run_name": f"Repair JSON ({schema_type}) - {request_id}"
        }

        # Adiciona timeout para opera√ß√£o de reparo
        repaired = await asyncio.wait_for(
            repair_chain.ainvoke(
                {
                    "malformed_json": json.dumps(malformed_output, ensure_ascii=False, indent=2),
                    "error": str(validation_error),
                    "transcript_preview": sanitize_transcript_for_log(normalized.transcript, 500),
                    "expected_schema": expected_schema
                },
                config=repair_trace_config
            ),
            timeout=REPAIR_TIMEOUT_SECONDS
        )
        
        logger.info(
            f"[REPAIR] [{request_id}] ‚úÖ JSON reparado com sucesso | "
            f"schema_type={schema_type}"
        )
        return repaired
        
    except asyncio.TimeoutError:
        logger.error(
            f"[REPAIR] [{request_id}] ‚è±Ô∏è Timeout na opera√ß√£o de reparo | "
            f"timeout={REPAIR_TIMEOUT_SECONDS}s"
        )
        raise Exception(f"Timeout na opera√ß√£o de reparo: {REPAIR_TIMEOUT_SECONDS}s")
    
    except Exception as e:
        logger.error(
            f"[REPAIR] [{request_id}] ‚ùå Erro durante reparo | "
            f"erro={type(e).__name__}: {str(e)[:200]}"
        )
        raise


def extract_and_record_token_usage(raw_response: Any, model: str, request_id: str) -> None:
    """
    Extrai informa√ß√µes de uso de tokens da resposta do LLM e registra nas m√©tricas Prometheus.
    
    Esta fun√ß√£o tenta m√∫ltiplos m√©todos para acessar os dados de token usage,
    j√° que diferentes vers√µes do LangChain podem expor esses dados em estruturas diferentes.
    
    Args:
        raw_response: Resposta bruta do LLM (antes do parser JSON)
        model: Nome do modelo OpenAI usado (ex: "gpt-4o")
        request_id: ID de correla√ß√£o para logs
    """
    try:
        # Tenta extrair tokens da resposta RAW usando m√∫ltiplos m√©todos
        usage = None
        
        # M√©todo 1: response_metadata (LangChain padr√£o)
        if hasattr(raw_response, 'response_metadata'):
            usage = raw_response.response_metadata.get('token_usage')
        
        # M√©todo 2: usage_metadata (vers√µes mais recentes)
        elif hasattr(raw_response, 'usage_metadata'):
            usage = raw_response.usage_metadata
            
        if usage:
            # Extrai valores dos tokens (suporta dict ou objeto)
            if isinstance(usage, dict):
                prompt_tokens = usage.get('prompt_tokens', 0)
                completion_tokens = usage.get('completion_tokens', 0) 
                total_tokens = usage.get('total_tokens', 0)
            else:
                prompt_tokens = getattr(usage, 'prompt_tokens', 0)
                completion_tokens = getattr(usage, 'completion_tokens', 0)
                total_tokens = getattr(usage, 'total_tokens', 0)
                
            # Registra m√©tricas se houver dados v√°lidos
            if total_tokens > 0:
                record_openai_tokens(model, prompt_tokens, completion_tokens, total_tokens)
                logger.debug(
                    f"[{request_id}] Tokens registrados: {total_tokens} total "
                    f"(prompt: {prompt_tokens}, completion: {completion_tokens})"
                )
            else:
                logger.debug(f"[{request_id}] Token usage encontrado mas valores s√£o zero")
        else:
            logger.debug(f"[{request_id}] Token usage n√£o encontrado na resposta")
            
    except Exception as e:
        logger.error(f"[{request_id}] Erro ao extrair tokens: {e}")
        # N√£o falha a opera√ß√£o por causa de m√©tricas


def sanitize_transcript_for_log(transcript: str, max_chars: int = 300) -> str:
    """
    Trunca a transcri√ß√£o para log seguro (sem PII completa).
    
    Esta fun√ß√£o √© usada para limitar o tamanho de transcri√ß√µes nos logs,
    evitando expor dados pessoais completos e mantendo os logs leg√≠veis.
    
    Args:
        transcript: Texto completo da transcri√ß√£o
        max_chars: N√∫mero m√°ximo de caracteres a manter (padr√£o: 300)
    
    Returns:
        str: Transcri√ß√£o truncada com indicador de truncamento
    
    Example:
        >>> long_text = "Cliente: Bom dia..." * 100
        >>> sanitize_transcript_for_log(long_text, 50)
        "Cliente: Bom dia...Cliente: Bom dia...Cliente: Bo... (truncado, total: 1500 chars)"
    """
    if len(transcript) <= max_chars:
        return transcript
    return transcript[:max_chars] + f"... (truncado, total: {len(transcript)} chars)"


def prepare_metadata_for_prompt(normalized: NormalizedInput) -> str:
    """
    Prepara os metadados para envio ao LLM em formato JSON leg√≠vel.
    
    Converte os campos de metadados do NormalizedInput em um JSON formatado,
    removendo valores None para clareza. O LLM ver√° apenas os dados fornecidos.
    
    Args:
        normalized: Dados normalizados da reuni√£o
    
    Returns:
        str: JSON formatado dos metadados (excluindo transcript)
    
    Example:
        >>> normalized = NormalizedInput(
        ...     transcript="...",
        ...     meeting_id="MTG123",
        ...     customer_id="CUST456",
        ...     customer_name=None  # Ser√° removido do JSON
        ... )
        >>> prepare_metadata_for_prompt(normalized)
        '{"meeting_id": "MTG123", "customer_id": "CUST456"}'
    """
    metadata_dict = {
        "meeting_id": normalized.meeting_id,
        "customer_id": normalized.customer_id,
        "customer_name": normalized.customer_name,
        "banker_id": normalized.banker_id,
        "banker_name": normalized.banker_name,
        "meet_type": normalized.meet_type,
        "meet_date": normalized.meet_date.isoformat() if normalized.meet_date else None,
    }
    
    # Remove valores None para clareza (LLM ver√° apenas o que foi fornecido)
    metadata_dict = {k: v for k, v in metadata_dict.items() if v is not None}
    
    return json.dumps(metadata_dict, ensure_ascii=False, indent=2)
