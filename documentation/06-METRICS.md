# üìä Monitoramento com Prometheus (M√©tricas)

## üéØ Vis√£o Geral e Objetivo

Este documento detalha a implementa√ß√£o de um sistema de monitoramento de m√©tricas utilizando **Prometheus**. O objetivo √© obter visibilidade sobre a performance, custos, e sa√∫de operacional do microservi√ßo de extra√ß√£o.

As m√©tricas nos permitem:
- **Monitorar Custos:** Acompanhar o consumo de tokens e o custo estimado com a API da OpenAI.
- **Identificar Erros:** Rastrear a quantidade e os tipos de erros (ex: falhas na API, retentativas, timeouts).
- **Analisar Performance:** Medir a lat√™ncia das extra√ß√µes e o tempo de resposta da API.
- **Entender o Uso:** Observar a quantidade de requisi√ß√µes, tipos de reuni√µes e fontes de dados.

---

## üõ†Ô∏è Arquitetura e Tecnologias

### 1. Tecnologias Envolvidas

| Componente | Tecnologia | Fun√ß√£o |
|---|---|---|
| **Aplica√ß√£o** | FastAPI + Python | A API que processa as requisi√ß√µes de extra√ß√£o. |
| **Instrumenta√ß√£o**| `prometheus-client` | Biblioteca Python que coleta e exp√µe as m√©tricas. |
| **Endpoint** | `GET /metrics` | Rota na API que exibe as m√©tricas em formato de texto. |
| **Coleta** | Prometheus Server | (Opcional, Futuro) Ferramenta que l√™ e armazena o hist√≥rico das m√©tricas. |
| **Visualiza√ß√£o** | Grafana | (Opcional, Futuro) Ferramenta para criar dashboards e gr√°ficos. |

### 2. Onde os Dados S√£o Armazenados

> **Escopo da Fase 3:** As m√©tricas ser√£o armazenadas **em mem√≥ria** dentro da pr√≥pria aplica√ß√£o. Se a API for reiniciada, os dados s√£o perdidos. Isto √© ideal para desenvolvimento e observa√ß√£o em tempo real.

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ FASE 3 (O QUE VAMOS IMPLEMENTAR)                            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                             ‚îÇ
‚îÇ üìç Armazenamento IN-MEMORY (dentro da API)                  ‚îÇ
‚îÇ   - M√©tricas existem como vari√°veis Python na RAM.          ‚îÇ
‚îÇ   - Expostas em tempo real via endpoint GET /metrics.       ‚îÇ
‚îÇ   - ‚ö†Ô∏è Dados S√ÉO PERDIDOS se a API reiniciar.               ‚îÇ
‚îÇ                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

> **Setup de Produ√ß√£o (Futuro):** Para persist√™ncia, um **Prometheus Server** √© configurado para "raspar" (fazer scraping) o endpoint `/metrics` periodicamente, salvando os dados em um banco de dados de s√©ries temporais.

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ PRODU√á√ÉO (SETUP COMPLETO - FUTURO)                          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                             ‚îÇ
‚îÇ üìç Prometheus Server (Time-Series Database)                 ‚îÇ
‚îÇ   - Faz scraping do endpoint GET /metrics a cada X segundos.‚îÇ
‚îÇ   - Armazena o hist√≥rico em disco.                          ‚îÇ
‚îÇ   - Permite consultas (PromQL), alertas e dashboards.       ‚îÇ
‚îÇ   - üéØ Dados PERSISTEM mesmo que a API reinicie.            ‚îÇ
‚îÇ                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìà M√©tricas Coletadas (As 12 M√©tricas)

A tabela abaixo detalha todas as m√©tricas implementadas, incluindo onde s√£o coletadas e como s√£o calculadas.

### ü§ñ M√©tricas da OpenAI API

| M√©trica | Tipo | Descri√ß√£o | Labels | Localiza√ß√£o | Como √© Calculada |
|---|---|---|---|---|---|
| `openai_requests_total` | Counter | Total de requisi√ß√µes √† OpenAI | `model`, `status` | `extractor.py` | +1 a cada chamada `chain.ainvoke()` (sucesso/erro) |
| `openai_errors_total` | Counter | Erros espec√≠ficos da OpenAI | `error_type` | `extractor.py` | +1 quando `RateLimitError`, `APITimeoutError`, `APIError` |
| `openai_tokens_total` | Counter | Tokens processados pela OpenAI | `type` | `extractor.py` | Soma de `prompt_tokens`, `completion_tokens`, `total_tokens` |
| `openai_estimated_cost_usd` | Counter | Custo estimado em USD | `model` | `extractor.py` | `(prompt_tokens/1M √ó $5) + (completion_tokens/1M √ó $15)` |
| `openai_repair_attempts_total` | Counter | Tentativas de reparo de JSON | `status` | `extractor.py` | +1 quando `_repair_json()` √© chamado (success/failed) |

### ‚ö° M√©tricas de Extra√ß√£o e Performance

| M√©trica | Tipo | Descri√ß√£o | Labels | Localiza√ß√£o | Como √© Calculada |
|---|---|---|---|---|---|
| `extraction_duration_seconds` | Histogram | Tempo de dura√ß√£o da extra√ß√£o | - | `main.py` | `time.time()` antes/depois de `extract_meeting_chain()` |
| `meetings_extracted_total` | Counter | Reuni√µes extra√≠das com sucesso | `source` | `main.py` | +1 quando extra√ß√£o completa (raw_meeting/transcript) |
| `meetings_by_type_total` | Counter | Reuni√µes por tipo | `meeting_type` | `main.py` | +1 usando `extracted.meeting_type` ou "Unknown" |
| `transcript_size_bytes` | Histogram | Tamanho das transcri√ß√µes | - | `main.py` | `len(normalized.transcript.encode('utf-8'))` |

### üåê M√©tricas HTTP e Rate Limiting

| M√©trica | Tipo | Descri√ß√£o | Labels | Localiza√ß√£o | Como √© Calculada |
|---|---|---|---|---|---|
| `rate_limit_exceeded_total` | Counter | Rate limits excedidos | `endpoint` | `main.py` | +1 no handler `RateLimitExceeded` |
| `api_errors_total` | Counter | Erros retornados pela API (502, 500) | `error_type`, `status_code` | `main.py` | +1 nos exception handlers (502/500) |
| `http_requests_total` | Counter | Requisi√ß√µes HTTP recebidas | `method`, `endpoint`, `status_code` | `main.py` (middleware) | +1 para cada requisi√ß√£o HTTP (sucesso/erro) |
| `http_requests_duration_seconds` | Histogram | Lat√™ncia das requisi√ß√µes HTTP | `method`, `endpoint` | `main.py` (middleware) | Timer no middleware para todas as rotas |

---

## üîÑ Fluxo de Coleta de M√©tricas

O fluxograma abaixo ilustra quando e como cada m√©trica √© coletada durante o ciclo de vida de uma requisi√ß√£o `POST /extract`.

```mermaid
flowchart TB
    Start([Cliente faz POST /extract]) --> Middleware[Middleware adiciona Request-ID]
    Middleware --> RateLimit{Rate Limit OK?}
    
    RateLimit -->|N√£o| RateLimitMetric[üî¢ rate_limit_exceeded_total++]
    RateLimitMetric --> Return429[Retorna 429]
    
    RateLimit -->|Sim| HTTPMetric1[üî¢ http_requests_total++]
    HTTPMetric1 --> Validation{Body v√°lido?}
    
    Validation -->|N√£o| ValidationError[Retorna 422]
    Validation -->|Sim| Extract[Chama extract_meeting_chain]
    
    Extract --> PreparePrompt[Prepara prompt para OpenAI]
    PreparePrompt --> CallOpenAI[üïí Timer inicia]
    
    CallOpenAI --> OpenAIRequest[Faz request √† OpenAI]
    OpenAIRequest --> OpenAIResponse{Sucesso?}
    
    OpenAIResponse -->|Erro| RetryCheck{Tentativa < 3?}
    RetryCheck -->|Sim| RetryMetric[üî¢ openai_repair_attempts_total++]
    RetryMetric --> CallOpenAI
    
    RetryCheck -->|N√£o| ErrorMetric[üî¢ openai_errors_total++<br/>type=timeout/rate_limit]
    ErrorMetric --> APIErrorMetric[üî¢ api_errors_total++<br/>error_type=openai_communication_error<br/>status_code=502]
    APIErrorMetric --> Return502[Retorna 502]
    
    OpenAIResponse -->|Sucesso| TimerStop[üïí Timer para]
    TimerStop --> ExtractTokens[Extrai response.usage.prompt_tokens<br/>response.usage.completion_tokens]
    
    ExtractTokens --> TokenMetrics[üî¢ openai_tokens_total += tokens<br/>üí∞ openai_estimated_cost_usd += custo]
    TokenMetrics --> SuccessMetrics[üî¢ openai_requests_total++<br/>üî¢ meetings_extracted_total++<br/>üìä extraction_duration_seconds.observe]
    
    SuccessMetrics --> HTTPMetric2[üî¢ http_requests_duration_seconds.observe]
    HTTPMetric2 --> Return200[Retorna 200 OK]
    
    subgraph "PROMETHEUS (Coleta)"
        direction LR
        PrometheusCheck{Scraping<br/>Ativo?}
        PrometheusCheck -->|Sim, a cada 15s| ExposeMetrics[GET /metrics<br/>retorna todas as m√©tricas<br/>em formato texto]
        ExposeMetrics --> PrometheusScrape[Prometheus Server<br/>armazena em<br/>time-series DB]
    PrometheusScrape --> Grafana[Grafana exibe<br/>em dashboards]
        PrometheusCheck -->|N√£o| StayInMemory[M√©tricas ficam<br/>em mem√≥ria Python]
    end
    
    Return200 --> PrometheusCheck
```

---

## üîß Detalhes T√©cnicos da Implementa√ß√£o

### Arquivos Envolvidos

| Arquivo | Responsabilidade | M√©tricas Coletadas |
|---|---|---|
| `app/metrics/collectors.py` | **Defini√ß√£o das m√©tricas** e fun√ß√µes auxiliares | Todas as 13 m√©tricas (defini√ß√£o) |
| `app/main.py` | **Instrumenta√ß√£o HTTP** e fluxo principal | `rate_limit_exceeded_total`, `api_errors_total`, `http_requests_total`, `extraction_duration_seconds`, `meetings_extracted_total`, `meetings_by_type_total`, `transcript_size_bytes`, `http_requests_duration_seconds` |
| `app/extractors/extractor.py` | **Instrumenta√ß√£o OpenAI** | `openai_requests_total`, `openai_errors_total`, `openai_tokens_total`, `openai_estimated_cost_usd`, `openai_repair_attempts_total` |
| **FastAPI Instrumentator** | **M√©tricas HTTP autom√°ticas** | M√©tricas adicionais do Prometheus (opcional) |

### Pontos de Coleta Espec√≠ficos

#### üìç Em `main.py` - Middleware `add_request_id_and_metrics()`

```python
# Linha ~205: Registra todas as requisi√ß√µes HTTP
record_http_request(
    method=request.method,
    endpoint=request.url.path,
    status_code=response.status_code
)

# Linha ~210: Registra dura√ß√£o de todas as requisi√ß√µes
record_http_duration(
    method=request.method,
    endpoint=request.url.path,
    duration=time.time() - start_time
)
```

#### üìç Em `main.py` - Exception Handlers

```python
# Handler de erros 502/500: Registra m√©trica de erro da API
record_api_error("openai_communication_error", 502)  # Para erros OpenAI
record_api_error("openai_invalid_response", 502)     # Para dados inv√°lidos
record_api_error("internal_error", 500)              # Para erros internos
```

#### üìç Em `main.py` - Fun√ß√£o `extract_meeting()`

```python
# Linha ~460: Timer HTTP iniciado
http_start_time = time.time()

# Linha ~487: Tamanho da transcri√ß√£o
transcript_size = len(normalized.transcript.encode('utf-8'))
record_transcript_size(transcript_size)

# Linha ~511: Timer de extra√ß√£o iniciado
extraction_start = time.time()
extracted = await extract_meeting_chain(...)

# Linha ~520: Dura√ß√£o da extra√ß√£o registrada
extraction_duration = time.time() - extraction_start
extraction_duration_seconds.observe(extraction_duration)

# Linha ~538: Reuni√£o extra√≠da registrada
source = "raw_meeting" if body.raw_meeting else "transcript"
meeting_type = extracted.meeting_type or "Unknown"
record_meeting_extracted(source, meeting_type)

# Linha ~542: Dura√ß√£o HTTP total registrada
http_duration = time.time() - http_start_time
record_http_duration("POST", "/extract", http_duration)
```

#### üìç Em `main.py` - Handler `rate_limit_exception_handler()`

```python
# Linha ~270: Rate limit excedido
endpoint = request.url.path
record_rate_limit_exceeded(endpoint)
```

#### üìç Em `extractor.py` - Fun√ß√£o `extract_meeting_chain()`

```python
# Linha ~366: Sucesso da OpenAI
model = get_model_from_env()
record_openai_request(model, "success")

# Linha ~370-400: Extra√ß√£o de tokens (m√∫ltiplos m√©todos)
# M√©todo 1: response_metadata
if hasattr(result, 'response_metadata') and result.response_metadata:
    usage = result.response_metadata.get('token_usage')
# M√©todo 2: usage direto
elif hasattr(result, 'usage') and result.usage:
    usage = result.usage
# M√©todo 3: response.usage (se dispon√≠vel)
elif hasattr(result, 'response') and hasattr(result.response, 'usage'):
    usage = result.response.usage

if usage:
    record_openai_tokens(model, prompt_tokens, completion_tokens, total_tokens)

# Linha ~379: Erro da OpenAI
record_openai_request(model, "error")
record_openai_error(type(e).__name__)

# Linha ~425: Reparo bem-sucedido
record_repair_attempt("success")

# Linha ~434: Reparo falhado
record_repair_attempt("failed")
```

### C√°lculos Espec√≠ficos

#### üí∞ Custo da OpenAI (`openai_estimated_cost_usd`)

```python
def calculate_openai_cost(model: str, prompt_tokens: int, completion_tokens: int) -> float:
    pricing = {
        "gpt-4o": {"input": 5.00, "output": 15.00},      # $/1M tokens
        "gpt-4o-mini": {"input": 0.15, "output": 0.60},
        "gpt-3.5-turbo": {"input": 0.50, "output": 1.50}
    }
    
    model_pricing = pricing.get(model, pricing["gpt-4o"])
    input_cost = (prompt_tokens / 1_000_000) * model_pricing["input"]
    output_cost = (completion_tokens / 1_000_000) * model_pricing["output"]
    
    return input_cost + output_cost
```

**Exemplo:** Para GPT-4o com 1000 prompt tokens + 500 completion tokens:
- Input: `(1000 / 1_000_000) √ó $5.00 = $0.005`
- Output: `(500 / 1_000_000) √ó $15.00 = $0.0075`
- **Total: $0.0125**

#### üìè Tamanho da Transcri√ß√£o (`transcript_size_bytes`)

```python
# Calcula bytes UTF-8 (n√£o caracteres)
transcript_size = len(normalized.transcript.encode('utf-8'))
```

**Diferen√ßa importante:** Caracteres especiais (acentos, emojis) ocupam mais de 1 byte em UTF-8.

#### ‚è±Ô∏è Dura√ß√£o da Extra√ß√£o (`extraction_duration_seconds`)

```python
# Timer espec√≠fico apenas para a chamada √† OpenAI
extraction_start = time.time()
extracted = await extract_meeting_chain(normalized, request_id)
extraction_duration = time.time() - extraction_start
```

**N√£o inclui:** Valida√ß√£o Pydantic, normaliza√ß√£o, logs. **Inclui:** Apenas o tempo de processamento do LLM.

### Configura√ß√£o de Buckets dos Histogramas

#### `extraction_duration_seconds`
```python
buckets=[0.5, 1.0, 2.0, 5.0, 10.0, 30.0, 60.0, float("inf")]
```
- **0.5s-2s:** Respostas r√°pidas (transcri√ß√µes curtas)
- **2s-10s:** Respostas normais (transcri√ß√µes m√©dias)
- **10s-30s:** Respostas lentas (transcri√ß√µes longas)
- **30s+:** Casos extremos ou problemas de rede

#### `transcript_size_bytes`
```python
buckets=[1000, 5000, 10000, 25000, 50000, 100000, 250000, float("inf")]
```
- **1KB-5KB:** Reuni√µes curtas (5-10 min)
- **5KB-25KB:** Reuni√µes m√©dias (15-30 min)
- **25KB-100KB:** Reuni√µes longas (45-90 min)
- **100KB+:** Reuni√µes muito longas (2h+)

#### `http_requests_duration_seconds`
```python
buckets=[0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0, float("inf")]
```
- **0.1s-0.5s:** Requisi√ß√µes muito r√°pidas (cache, valida√ß√£o simples)
- **0.5s-2.5s:** Requisi√ß√µes normais
- **2.5s-10s:** Requisi√ß√µes lentas (processamento complexo)
- **10s+:** Timeouts ou problemas

---

## ‚öôÔ∏è Como Funciona Cada Tipo de M√©trica

### 1. Counter (Contador)
√â um valor que **s√≥ pode aumentar** (ou ser resetado para zero). Ideal para contar eventos, como n√∫mero de requisi√ß√µes, erros, ou total de tokens.

```python
# Declara√ß√£o (feita uma vez no in√≠cio)
from prometheus_client import Counter
erros_totais = Counter('erros_totais', 'Total de erros ocorridos')

# Uso (a cada vez que um erro acontece)
erros_totais.inc()  # Incrementa o contador em 1
```

### 2. Gauge (Medidor)
Representa um valor num√©rico que pode **subir e descer**. Perfeito para medir valores instant√¢neos, como n√∫mero de usu√°rios ativos, uso de mem√≥ria ou temperatura.

```python
# Declara√ß√£o
from prometheus_client import Gauge
conexoes_ativas = Gauge('conexoes_ativas', 'N√∫mero de conex√µes ativas')

# Uso
conexoes_ativas.inc()  # +1 quando uma conex√£o √© aberta
conexoes_ativas.dec()  # -1 quando uma conex√£o √© fechada
```

### 3. Histogram (Histograma)
Mede a **distribui√ß√£o de valores** em "baldes" (buckets) configur√°veis. √â a melhor forma de medir lat√™ncia e tamanhos de requests. Automaticamente calcula a contagem (`_count`), a soma (`_sum`), e os buckets (`_bucket`).

Com esses dados, o Prometheus pode calcular **percentis** (ex: P95, P99), que s√£o cruciais para entender a performance real da aplica√ß√£o.

```python
# Declara√ß√£o com buckets de tempo
from prometheus_client import Histogram
latencia_request = Histogram(
    'latencia_request_seconds',
    'Lat√™ncia das requisi√ß√µes',
    buckets=[0.1, 0.5, 1, 2.5, 5, 10]  # buckets em segundos
)

# Uso
import time
start = time.time()
# ... processa a requisi√ß√£o ...
duration = time.time() - start
latencia_request.observe(duration)  # Ex: registra um valor de 0.78s
```

---

## üìä Configura√ß√£o dos Histogramas

Para as m√©tricas do tipo **Histogram**, utilizaremos os seguintes buckets otimizados:

### `extraction_duration_seconds`
```python
buckets=[0.5, 1.0, 2.0, 5.0, 10.0, 30.0, 60.0, float("inf")]
```
- **Justificativa:** Extra√ß√µes t√≠picas variam de 0.5s a 30s. Buckets cobrem desde respostas r√°pidas at√© casos extremos.

### `transcript_size_bytes`
```python
buckets=[1000, 5000, 10000, 25000, 50000, 100000, 250000, float("inf")]
```
- **Justificativa:** Transcri√ß√µes variam de ~1KB (reuni√µes curtas) at√© ~250KB (reuni√µes longas de 2h+).

### `http_requests_duration_seconds`
```python
buckets=[0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0, float("inf")]
```
- **Justificativa:** Requisi√ß√µes HTTP devem ser r√°pidas. Buckets focam em lat√™ncias baixas com alguns outliers.

---

## üìã Exemplo de Sa√≠da do Endpoint `/metrics`

Ap√≥s algumas requisi√ß√µes, o endpoint `GET /metrics` retornar√° algo similar a:

```
# HELP openai_requests_total Total de requisi√ß√µes feitas √† API da OpenAI
# TYPE openai_requests_total counter
openai_requests_total{model="gpt-4o",status="success"} 15.0
openai_requests_total{model="gpt-4o",status="error"} 2.0

# HELP openai_tokens_total Contagem total de tokens processados pela OpenAI
# TYPE openai_tokens_total counter
openai_tokens_total{type="prompt"} 7500.0
openai_tokens_total{type="completion"} 3200.0
openai_tokens_total{type="total"} 10700.0

# HELP openai_estimated_cost_usd Custo total estimado em USD baseado no uso de tokens
# TYPE openai_estimated_cost_usd counter
openai_estimated_cost_usd{model="gpt-4o"} 0.05125

# HELP extraction_duration_seconds Distribui√ß√£o do tempo de dura√ß√£o da extra√ß√£o
# TYPE extraction_duration_seconds histogram
extraction_duration_seconds_bucket{le="0.5"} 2.0
extraction_duration_seconds_bucket{le="1.0"} 8.0
extraction_duration_seconds_bucket{le="2.0"} 12.0
extraction_duration_seconds_bucket{le="5.0"} 15.0
extraction_duration_seconds_bucket{le="10.0"} 15.0
extraction_duration_seconds_bucket{le="30.0"} 15.0
extraction_duration_seconds_bucket{le="60.0"} 15.0
extraction_duration_seconds_bucket{le="+Inf"} 15.0
extraction_duration_seconds_sum 18.75
extraction_duration_seconds_count 15.0

# HELP meetings_extracted_total N√∫mero total de reuni√µes extra√≠das com sucesso
# TYPE meetings_extracted_total counter
meetings_extracted_total{source="transcript"} 10.0
meetings_extracted_total{source="raw_meeting"} 5.0

# HELP http_requests_total Total de requisi√ß√µes HTTP recebidas pela API
# TYPE http_requests_total counter
http_requests_total{method="POST",endpoint="/extract",status_code="200"} 15.0
http_requests_total{method="POST",endpoint="/extract",status_code="422"} 3.0
http_requests_total{method="POST",endpoint="/extract",status_code="502"} 2.0
http_requests_total{method="GET",endpoint="/metrics",status_code="200"} 8.0

# HELP api_errors_total Total de erros retornados pela API (502 Bad Gateway, 500 Internal Server Error)
# TYPE api_errors_total counter
api_errors_total{error_type="openai_communication_error",status_code="502"} 2.0
api_errors_total{error_type="openai_invalid_response",status_code="502"} 1.0
api_errors_total{error_type="internal_error",status_code="500"} 0.0
```

---

## üîß Corre√ß√µes Implementadas (v1.1)

### Problemas Identificados e Solu√ß√µes

#### 1. **M√©tricas de Tokens N√£o Funcionando**
**Problema**: O c√≥digo anterior tentava acessar `token_usage` apenas via `response_metadata`, mas o LangChain pode expor os dados em diferentes estruturas.

**Solu√ß√£o**: Implementamos m√∫ltiplas formas de acessar os dados de tokens:
```python
# M√©todo 1: response_metadata
if hasattr(result, 'response_metadata') and result.response_metadata:
    usage = result.response_metadata.get('token_usage')

# M√©todo 2: usage direto  
elif hasattr(result, 'usage') and result.usage:
    usage = result.usage
    
# M√©todo 3: response.usage (se dispon√≠vel)
elif hasattr(result, 'response') and hasattr(result.response, 'usage'):
    usage = result.response.usage
```

#### 2. **`http_requests_total` N√£o Capturava Todas as Requisi√ß√µes**
**Problema**: O instrumentador FastAPI n√£o estava configurado corretamente para capturar todas as requisi√ß√µes HTTP.

**Solu√ß√£o**: Modificamos o middleware `add_request_id` para `add_request_id_and_metrics`:
```python
@app.middleware("http")
async def add_request_id_and_metrics(request: Request, call_next):
    # Registra todas as requisi√ß√µes HTTP (sucesso e erro)
    record_http_request(method=request.method, endpoint=request.url.path, status_code=response.status_code)
    record_http_duration(method=request.method, endpoint=request.url.path, duration=duration)
```

#### 3. **Falta de Visibilidade sobre Erros 502/500**
**Problema**: N√£o havia m√©tricas espec√≠ficas para monitorar quantas requisi√ß√µes estavam retornando erros da API.

**Solu√ß√£o**: Criamos a nova m√©trica `api_errors_total`:
```python
api_errors_total = Counter(
    'api_errors_total',
    'Total de erros retornados pela API (502 Bad Gateway, 500 Internal Server Error)',
    ['error_type', 'status_code']
)

# Registrada nos exception handlers:
record_api_error("openai_communication_error", 502)  # Erros OpenAI
record_api_error("openai_invalid_response", 502)     # Dados inv√°lidos  
record_api_error("internal_error", 500)              # Erros internos
```

### Resultado das Corre√ß√µes

| M√©trica | Antes | Depois |
|---------|-------|--------|
| `openai_tokens_total` | ‚ùå Vazia | ‚úÖ Funcionando |
| `http_requests_total` | ‚ùå Vazia | ‚úÖ Funcionando |
| `api_errors_total` | ‚ùå N√£o existia | ‚úÖ Nova m√©trica |

---

## ‚úÖ Escopo da Implementa√ß√£o (Fase 3)

Para esta fase, o foco √© instrumentar a aplica√ß√£o e expor as m√©tricas.

- **[X] Instrumentar a aplica√ß√£o:** Adicionar as bibliotecas `prometheus-client` e `prometheus-fastapi-instrumentator`.
- **[X] Criar Coletores:** Definir as 13 m√©tricas customizadas listadas acima.
- **[X] Registrar M√©tricas:** Inserir os comandos (`.inc()`, `.observe()`) nos locais corretos do c√≥digo.
- **[X] Expor Endpoint:** Criar a rota `GET /metrics` para que as m√©tricas possam ser lidas.
- **[ ] N√ÉO Instalar Servidor:** A instala√ß√£o e configura√ß√£o do Prometheus Server e Grafana n√£o fazem parte desta fase.

---

## üñ•Ô∏è Como Acessar as M√©tricas

Ap√≥s a implementa√ß√£o e com a API rodando, as m√©tricas estar√£o dispon√≠veis em tempo real.

1.  **Inicie a API:**
    ```bash
    uvicorn app.main:app --reload
    ```
2.  **Acesse no Navegador ou via cURL:**
    - URL: `http://localhost:8000/metrics`
    - Comando: `curl http://localhost:8000/metrics`

Voc√™ ver√° uma sa√≠da de texto puro com todas as m√©tricas e seus valores atuais, pronta para ser consumida por um servidor Prometheus.

---

## üîß Troubleshooting e Dicas

### Problemas Comuns

**1. Endpoint `/metrics` retorna 404:**
- Verifique se a instrumenta√ß√£o foi inicializada no `main.py`
- Confirme que `prometheus-fastapi-instrumentator` est√° instalado

**2. M√©tricas aparecem como 0 ou n√£o aparecem:**
- Verifique se os coletores est√£o sendo importados corretamente
- Confirme que os `.inc()` e `.observe()` est√£o sendo chamados
- Teste fazendo algumas requisi√ß√µes para `/extract`

**3. Valores de custo incorretos:**
- Verifique se os pre√ßos do modelo est√£o atualizados
- Confirme que `response.usage` est√° dispon√≠vel na resposta da OpenAI

### Comandos √öteis para Teste

```bash
# Fazer uma requisi√ß√£o e ver as m√©tricas
curl -X POST http://localhost:8000/extract -H "Content-Type: application/json" -d '{"transcript":"teste"}'
curl http://localhost:8000/metrics | grep openai

# Filtrar m√©tricas espec√≠ficas
curl -s http://localhost:8000/metrics | grep -E "(openai|meetings|extraction)"

# Ver apenas contadores (sem histogramas)
curl -s http://localhost:8000/metrics | grep -v "_bucket\|_sum\|_count"

# Monitorar m√©tricas em tempo real
watch -n 2 'curl -s http://localhost:8000/metrics | grep -E "openai_requests_total|meetings_extracted_total"'

# Verificar custos acumulados
curl -s http://localhost:8000/metrics | grep "openai_estimated_cost_usd"

# Analisar performance (percentis)
curl -s http://localhost:8000/metrics | grep "extraction_duration_seconds"

# Monitorar erros da API em tempo real
watch -n 5 'curl -s http://localhost:8000/metrics | grep "api_errors_total"'

# Verificar taxa de erro (requisi√ß√µes que falharam)
curl -s http://localhost:8000/metrics | grep -E "http_requests_total.*status_code=\"(502|500)\""
```

### Interpreta√ß√£o dos Dados

- **Counters:** Sempre crescem. Para ver taxa por minuto, use ferramentas como Prometheus
- **Histogramas:** `_sum/_count` = m√©dia. `_bucket` = distribui√ß√£o percentual
- **Labels:** Permitem filtrar e agrupar m√©tricas (ex: por modelo, status, etc.)

---

## üìã Requisitos T√©cnicos e Considera√ß√µes

### Depend√™ncias Necess√°rias

```txt
prometheus-client==0.20.0           # Biblioteca core do Prometheus
prometheus-fastapi-instrumentator==7.0.0  # Integra√ß√£o com FastAPI
```

### Vari√°veis de Ambiente

| Vari√°vel | Padr√£o | Descri√ß√£o | Impacto nas M√©tricas |
|---|---|---|---|
| `ENABLE_METRICS` | `True` | Habilita/desabilita coleta de m√©tricas | Se `False`, endpoint `/metrics` retorna vazio |
| `OPENAI_MODEL` | `"gpt-4o"` | Modelo OpenAI usado | Afeta c√°lculo de custos e labels |
| `RATE_LIMIT_PER_MINUTE` | `"10"` | Limite de requisi√ß√µes por minuto | Afeta frequ√™ncia de `rate_limit_exceeded_total` |

### Performance e Overhead

#### Impacto na Performance
- **Counters:** ~0.001ms por incremento (desprez√≠vel)
- **Histogramas:** ~0.005ms por observa√ß√£o (desprez√≠vel)
- **Endpoint `/metrics`:** ~10-50ms para serializar todas as m√©tricas

#### Uso de Mem√≥ria
- **Por Counter:** ~200 bytes + (50 bytes √ó n√∫mero de labels √∫nicos)
- **Por Histogram:** ~1KB + (100 bytes √ó n√∫mero de buckets √ó labels √∫nicos)
- **Total estimado:** ~50KB para todas as m√©tricas com uso normal

#### Recomenda√ß√µes
- ‚úÖ **Mantenha habilitado em produ√ß√£o** - overhead √© m√≠nimo
- ‚ö†Ô∏è **Cuidado com labels de alta cardinalidade** (ex: IDs √∫nicos)
- ‚úÖ **Use ferramentas de monitoramento** para alertas autom√°ticos

### Limita√ß√µes Conhecidas

#### üîç Extra√ß√£o de Tokens
```python
# Nem sempre dispon√≠vel no LangChain
if hasattr(raw_output, 'response_metadata') and 'token_usage' in raw_output.response_metadata:
    # Tokens dispon√≠veis
else:
    # Tokens n√£o dispon√≠veis - m√©trica n√£o ser√° registrada
```

**Solu√ß√£o:** M√©tricas de custo podem estar subestimadas se tokens n√£o forem extra√≠veis.

#### üìä Precis√£o dos Buckets
- **Histogramas s√£o aproxima√ß√µes** - valores exatos ficam em `_sum` e `_count`
- **Buckets fixos** - n√£o se adaptam automaticamente √† distribui√ß√£o real
- **Percentis calculados** pelo Prometheus podem ter pequenas imprecis√µes

#### üîÑ Reset de M√©tricas
- **Reinicializa√ß√£o da API** zera todas as m√©tricas (armazenamento in-memory)
- **N√£o h√° persist√™ncia** entre restarts
- **Para produ√ß√£o:** Configure Prometheus Server para hist√≥rico

### Monitoramento Recomendado

#### üö® Alertas Cr√≠ticos
```yaml
# Exemplo de alertas Prometheus
- alert: HighOpenAIErrorRate
  expr: rate(openai_errors_total[5m]) > 0.1
  for: 2m
  
- alert: HighExtractionLatency
  expr: histogram_quantile(0.95, extraction_duration_seconds) > 30
  for: 5m
  
- alert: HighCostPerHour
  expr: rate(openai_estimated_cost_usd[1h]) > 10
  for: 10m
```

#### üìà Dashboards √öteis
- **Custo por hora/dia** - `rate(openai_estimated_cost_usd[1h]) * 3600`
- **Lat√™ncia P95** - `histogram_quantile(0.95, extraction_duration_seconds)`
- **Taxa de erro** - `rate(openai_errors_total[5m]) / rate(openai_requests_total[5m])`
- **Taxa de erro da API** - `rate(api_errors_total[5m]) / rate(http_requests_total[5m])`
- **Throughput** - `rate(meetings_extracted_total[1m])`
- **Erros 502** - `rate(api_errors_total{status_code="502"}[5m])`
- **Erros 500** - `rate(api_errors_total{status_code="500"}[5m])`

### Troubleshooting Avan√ßado

#### M√©tricas Zeradas Ap√≥s Restart
```bash
# Verificar se m√©tricas est√£o sendo coletadas
curl -s http://localhost:8000/metrics | grep -c "# HELP"  # Deve retornar 12+

# Fazer algumas requisi√ß√µes para gerar dados
for i in {1..5}; do
  curl -X POST http://localhost:8000/extract \
    -H "Content-Type: application/json" \
    -d '{"transcript":"Teste '$i'"}'
done

# Verificar se m√©tricas foram incrementadas
curl -s http://localhost:8000/metrics | grep "meetings_extracted_total"
```

#### Debug de Labels
```bash
# Ver todos os labels √∫nicos de uma m√©trica
curl -s http://localhost:8000/metrics | grep "openai_requests_total" | cut -d'{' -f2 | cut -d'}' -f1 | sort | uniq

# Verificar se labels est√£o corretos
curl -s http://localhost:8000/metrics | grep 'model="gpt-4o"'

# Verificar erros da API
curl -s http://localhost:8000/metrics | grep "api_errors_total"

# Verificar todas as requisi√ß√µes HTTP
curl -s http://localhost:8000/metrics | grep "http_requests_total"
```

---

## üìù Changelog

### v1.1.0 (2025-10-04) - Corre√ß√µes de M√©tricas

#### ‚úÖ Corre√ß√µes Implementadas
- **`openai_tokens_total`**: Corrigido para funcionar com m√∫ltiplos m√©todos de extra√ß√£o de tokens do LangChain
- **`http_requests_total`**: Corrigido middleware para capturar todas as requisi√ß√µes HTTP (sucesso e erro)
- **`api_errors_total`**: Nova m√©trica para monitorar erros 502/500 da API
- **Logging**: Melhorado com informa√ß√µes detalhadas sobre tokens registrados
- **Endpoint `/metrics`**: Removido endpoint duplicado

#### üîß Melhorias T√©cnicas
- Middleware `add_request_id` renomeado para `add_request_id_and_metrics`
- M√∫ltiplas formas de acessar dados de tokens (response_metadata, usage direto, response.usage)
- Registro de erros 502/500 em todos os exception handlers
- Logs mais informativos para debugging de m√©tricas

#### üìä Status das M√©tricas
| M√©trica | v1.0.0 | v1.1.0 |
|---------|--------|--------|
| `openai_tokens_total` | ‚ùå N√£o funcionava | ‚úÖ Funcionando |
| `http_requests_total` | ‚ùå N√£o funcionava | ‚úÖ Funcionando |
| `api_errors_total` | ‚ùå N√£o existia | ‚úÖ Nova m√©trica |
| `openai_estimated_cost_usd` | ‚ùå N√£o funcionava | ‚úÖ Funcionando |
| Demais m√©tricas | ‚úÖ Funcionando | ‚úÖ Funcionando |

---

**√öltima atualiza√ß√£o:** 2025-10-04  
**Vers√£o:** 1.1.0  
**Status:** ‚úÖ Produ√ß√£o com corre√ß√µes implementadas