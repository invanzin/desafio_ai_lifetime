# üß† Documenta√ß√£o: Analyzer (analyzer.py)

## üìö √çndice

1. [Vis√£o Geral](#vis√£o-geral)
2. [Arquitetura LangChain](#arquitetura-langchain)
3. [Componentes Principais](#componentes-principais)
4. [Fluxo de An√°lise](#fluxo-de-an√°lise)
5. [Valida√ß√£o de Sentimento](#valida√ß√£o-de-sentimento)
6. [Resili√™ncia e Retry](#resili√™ncia-e-retry)
7. [Sistema de Reparo](#sistema-de-reparo)
8. [Logging e Seguran√ßa](#logging-e-seguran√ßa)
9. [Exemplos Pr√°ticos](#exemplos-pr√°ticos)

---

## üéØ Vis√£o Geral

O arquivo `analyzer.py` √© o **c√©rebro da an√°lise de sentimento** do microservi√ßo. Ele √© respons√°vel por:

‚úÖ Conectar com a OpenAI API (GPT-4o/GPT-4-turbo)  
‚úÖ Analisar sentimento (positive/neutral/negative)  
‚úÖ Calcular score num√©rico (0.0-1.0)  
‚úÖ Garantir consist√™ncia label ‚Üî score  
‚úÖ Identificar riscos e insights estrat√©gicos  
‚úÖ Resili√™ncia com retry autom√°tico  
‚úÖ Reparar JSONs malformados  
‚úÖ Validar sa√≠da com Pydantic  
‚úÖ Logar tudo de forma segura (sem PII completa)

### Por que Temperature 0.2?

Ao contr√°rio do **Extractor** (que usa `temperature=0` para ser determin√≠stico), o **Analyzer** usa **`temperature=0.2`** porque:

- **An√°lise de sentimento √© subjetiva:** Permite certa varia√ß√£o na interpreta√ß√£o emocional
- **Insights criativos:** Temperatura baixa ainda mant√©m consist√™ncia, mas permite insights mais elaborados
- **Balan√ßo ideal:** 0.2 √© baixo o suficiente para evitar "alucina√ß√µes", mas alto o suficiente para an√°lises matizadas

**Compara√ß√£o:**
```python
# temperature=0 (EXTRACTOR - puramente determin√≠stico)
"Cliente demonstrou interesse"
"Cliente demonstrou interesse"  # Sempre 100% igual

# temperature=0.2 (ANALYZER - levemente criativo)
"Cliente demonstrou otimismo moderado com reservas t√©cnicas"
"Cliente mostrou interesse positivo com preocupa√ß√µes sobre prazo"
# Similar, mas permite varia√ß√µes sutis na an√°lise
```

---

## üèóÔ∏è Arquitetura LangChain

### Componentes da Chain

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    LANGCHAIN CHAIN                       ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ   Prompt     ‚îÇ ‚Üí ‚îÇ     LLM      ‚îÇ ‚Üí ‚îÇ   Parser    ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  Template    ‚îÇ   ‚îÇ  (ChatGPT)   ‚îÇ   ‚îÇ   (JSON)    ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Input: transcript + metadata_json                       ‚îÇ
‚îÇ  Output: dict Python com sentiment_label + score        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### C√≥digo Real

```python
# 1. Prompt Template (carregado do LangChain Hub)
prompt_hub_name = os.getenv("ANALYZER_PROMPT_HUB_NAME")
prompt = hub.pull(prompt_hub_name)
# Exemplo: "ivan-furukawa/analyzer-sentimento-bancario"

# 2. LLM (ChatGPT) com temperatura LIGEIRAMENTE CRIATIVA
llm = default_client.get_llm(temperature=0.2)  # ‚Üê Diferen√ßa do Extractor!

# 3. Parser JSON
parser = JsonOutputParser()

# 4. Chain RAW (para capturar tokens)
chain_raw = prompt | llm

# 5. Invoca√ß√£o (ass√≠ncrona)
raw_response = await chain_raw.ainvoke({
    "transcript": "Cliente: Estou muito satisfeito...",
    "metadata_json": '{"meeting_id": "MTG001"}'
})

# Parse manual do JSON
result = parser.parse(raw_response.content)

# Resultado: dict Python com an√°lise completa!
{
    "sentiment_label": "positive",
    "sentiment_score": 0.85,
    "summary": "Reuni√£o positiva... (150 palavras)",
    "key_points": [...],
    "risks": [...]
}
```

---

## üß© Componentes Principais

### 1. Inicializa√ß√£o do LLM

```python
from llm.openai_client import default_client

llm = default_client.get_llm(temperature=0.2)
```

**Configura√ß√£o Completa (via `openai_client.py`):**
```python
ChatOpenAI(
    model="gpt-4o",           # Modelo padr√£o (env var)
    temperature=0.2,          # Levemente criativo
    timeout=30.0,             # Timeout de 30s
    api_key=os.getenv("OPENAI_API_KEY"),
    model_kwargs={
        "stream_usage": True  # Captura tokens para m√©tricas
    }
)
```

**Por que `temperature=0.2` √© ideal?**

| Temperature | Comportamento | Uso Ideal |
|-------------|---------------|-----------|
| 0.0 | 100% determin√≠stico | Extra√ß√£o de dados estruturados |
| **0.2** | **Levemente criativo** | **An√°lise de sentimento** |
| 0.7 | Criativo e variado | Escrita criativa, brainstorm |
| 1.0 | Muito aleat√≥rio | Gera√ß√£o de ideias, poesia |

---

### 2. Prompt System (LangChain Hub)

Ao contr√°rio do Extractor (que define o prompt em c√≥digo), o Analyzer **carrega o prompt do LangChain Hub**:

```python
prompt_hub_name = os.getenv("ANALYZER_PROMPT_HUB_NAME")
# Exemplo: "ivan-furukawa/analyzer-sentimento-bancario"

try:
    logger.info(f"üîÑ Carregando prompt do Hub: {prompt_hub_name}")
    prompt = hub.pull(prompt_hub_name)
    logger.debug(f"‚úÖ Prompt do Analyzer carregado com sucesso")
except Exception as e:
    logger.error(f"‚ùå Falha ao carregar o prompt: {e}")
    raise
```

**Benef√≠cios do LangChain Hub:**
- ‚úÖ **Versionamento de prompts:** Atualizar sem mexer no c√≥digo
- ‚úÖ **A/B testing:** Testar diferentes prompts facilmente
- ‚úÖ **Colabora√ß√£o:** Compartilhar prompts entre times
- ‚úÖ **Rollback r√°pido:** Reverter para vers√µes anteriores

**Estrutura T√≠pica do Prompt:**
```
Voc√™ √© um assistente especializado em an√°lise de sentimento de reuni√µes banc√°rias.

**INSTRU√á√ïES:**

1. **AN√ÅLISE DE SENTIMENTO:**
   - Classifique o sentimento geral em: "positive", "neutral", ou "negative"
   - Calcule um score num√©rico entre 0.0 e 1.0:
     * 0.0-0.4: negative (cliente insatisfeito, preocupado, frustrado)
     * 0.4-0.6: neutral (cliente neutro, sem emo√ß√µes fortes)
     * 0.6-1.0: positive (cliente satisfeito, entusiasmado, confiante)

2. **CONSIST√äNCIA OBRIGAT√ìRIA:**
   - sentiment_label = "positive" ‚Üí sentiment_score >= 0.6
   - sentiment_label = "neutral" ‚Üí 0.4 <= sentiment_score < 0.6
   - sentiment_label = "negative" ‚Üí sentiment_score < 0.4

3. **IDENTIFICA√á√ÉO DE RISCOS:**
   - Identifique preocupa√ß√µes, obje√ß√µes ou sinais de alerta
   - Se n√£o houver riscos identific√°veis, retorne lista vazia []

4. **FORMATO DE SA√çDA:**
Retorne um JSON v√°lido:
{
  "sentiment_label": "positive|neutral|negative",
  "sentiment_score": 0.85,
  "summary": "Resumo de 100-200 palavras...",
  "key_points": ["Ponto 1", "Ponto 2"],
  "action_items": ["A√ß√£o 1", "A√ß√£o 2"],
  "risks": ["Risco 1"] ou []
}
```

---

### 3. Fun√ß√µes Auxiliares

O Analyzer **reutiliza fun√ß√µes compartilhadas** do m√≥dulo `utils`:

```python
from utils import (
    sanitize_transcript_for_log,    # Prote√ß√£o de PII
    prepare_metadata_for_prompt,    # Formata√ß√£o de metadata
    repair_json,                    # Reparo de JSON
    extract_and_record_token_usage, # M√©tricas de tokens
    log_retry_attempt               # Log de retry
)
```

#### `sanitize_transcript_for_log()` - Prote√ß√£o de PII

```python
transcript = "Cliente Jo√£o Silva, CPF 123.456.789-00, est√° muito satisfeito..."

# Log SEGURO ‚úÖ
logger.info(f"Transcript: {sanitize_transcript_for_log(transcript, 100)}")
# Output: "Cliente Jo√£o Silva, CPF 123.456.789-00, est√° muito sa... (truncado, total: 523 chars)"
```

#### `prepare_metadata_for_prompt()` - Formata√ß√£o de Metadata

```python
normalized = NormalizedInput(
    transcript="...",
    meeting_id="MTG001",
    customer_id="CUST001",
    meet_date=datetime(2025, 9, 10, 14, 30)
)

metadata_json = prepare_metadata_for_prompt(normalized)
# Resultado:
# {
#   "meeting_id": "MTG001",
#   "customer_id": "CUST001",
#   "meet_date": "2025-09-10T14:30:00"
# }
```

---

## üîÑ Fluxo de An√°lise

### Fun√ß√£o Principal: `analyze_sentiment_chain()`

```python
@retry(
    reraise=True,
    stop=stop_after_attempt(3),  # M√°ximo 3 tentativas
    wait=wait_exponential(multiplier=0.5, min=0.5, max=5.0),
    retry=retry_if_exception_type((RateLimitError, APITimeoutError, APIError)),
    before_sleep=log_retry_attempt,  # Log antes de cada retry
)
async def analyze_sentiment_chain(
    normalized: NormalizedInput,
    request_id: str = "-"
) -> AnalyzedMeeting:
    """
    Analisa sentimento e gera insights de uma reuni√£o usando OpenAI + LangChain.
    
    Orquestra todo o processo de an√°lise:
    1. Valida dados de entrada
    2. Prepara os dados para o prompt
    3. Chama o LLM com retry autom√°tico (at√© 3 tentativas)
    4. Extrai tokens e registra m√©tricas Prometheus
    5. Valida o resultado com Pydantic (consist√™ncia label ‚Üî score)
    6. Tenta reparar se a valida√ß√£o falhar
    7. Preenche a chave de idempot√™ncia
    8. Retorna o objeto validado
    """
```

### Passo a Passo Detalhado

#### Passo 1: Prepara√ß√£o

```python
# Log in√≠cio (sem PII completa)
logger.info(
    f"[ANALYZE] [{request_id}] üß† Iniciando an√°lise de sentimento | "
    f"transcript_len={len(normalized.transcript)} | "
    f"has_metadata={'sim' if normalized.meeting_id else 'n√£o'}"
)

# Prepara metadados para o prompt
metadata_json = prepare_metadata_for_prompt(normalized)
metadata_fields_count = len(json.loads(metadata_json)) if metadata_json != '{}' else 0

logger.info(
    f"[{request_id}] ü§ñ Chamada √† OpenAI | "
    f"metadata_fields={metadata_fields_count} | "
    f"transcript_preview={sanitize_transcript_for_log(normalized.transcript, 100)}"
)
```

#### Passo 2: Chamada ao LLM (com captura de tokens)

```python
# Prepara configura√ß√£o para LangSmith (observabilidade)
trace_config = {
    "metadata": {
        "request_id": request_id,
        "transcript_length": len(normalized.transcript),
        "has_metadata_input": bool(metadata_fields_count > 0)
    },
    "run_name": f"Analyze Meeting - {request_id}"
}

# üî• Chain RAW para capturar metadados de tokens
llm_start = time.time()
raw_response = await chain_raw.ainvoke(
    {
        "transcript": normalized.transcript,
        "metadata_json": metadata_json
    },
    config=trace_config
)

# Registra m√©tricas Prometheus
model = get_model_from_env()  # Ex: "gpt-4o"
record_openai_request(model, "success")

# Extrai e registra tokens (prompt_tokens, completion_tokens)
extract_and_record_token_usage(raw_response, model, request_id)

# Parse manual do JSON
raw_output = parser.parse(raw_response.content)

llm_duration = time.time() - llm_start
logger.info(
    f"[RESPONSE] [{request_id}] ‚úÖ OpenAI API respondeu | "
    f"duration={llm_duration:.2f}s | "
    f"output_keys={list(raw_output.keys())}"
)
```

**Por que Chain RAW ao inv√©s de Chain com Parser?**

```python
# ‚ùå Chain com parser direto (n√£o captura tokens)
chain = prompt | llm | parser
result = await chain.ainvoke(...)
# Problema: `result` j√° √© dict, perdemos acesso aos metadados do LLM!

# ‚úÖ Chain RAW + parse manual (captura tokens)
chain_raw = prompt | llm
raw_response = await chain_raw.ainvoke(...)
# raw_response.usage.prompt_tokens ‚Üê Dispon√≠vel!
# raw_response.usage.completion_tokens ‚Üê Dispon√≠vel!
result = parser.parse(raw_response.content)
```

#### Passo 3: Valida√ß√£o com Pydantic

```python
try:
    analyzed = AnalyzedMeeting.model_validate(raw_output)
    logger.debug(f"[SUCCESS] [{request_id}] ‚úÖ Valida√ß√£o Pydantic OK")
    
    logger.info(
        f"[SUCCESS] [{request_id}] üéâ An√°lise conclu√≠da | "
        f"sentiment={analyzed.sentiment_label} | "
        f"score={analyzed.sentiment_score:.2f} | "
        f"summary_words={len(analyzed.summary.split())} | "
        f"risks={len(analyzed.risks)}"
    )

except Exception as validation_error:
    # Valida√ß√£o falhou ‚Üí tenta reparar UMA vez
    logger.warning(
        f"[VALIDATION] [{request_id}] ‚ö†Ô∏è Valida√ß√£o falhou | "
        f"erro={type(validation_error).__name__}: {str(validation_error)[:200]}"
    )
    
    repaired_output = await repair_json(
        malformed_output=raw_output,
        validation_error=str(validation_error),
        normalized=normalized,
        request_id=request_id,
        schema_type="analyze"  # ‚Üê Indica schema do Analyzer
    )
    
    # Tenta validar novamente
    analyzed = AnalyzedMeeting.model_validate(repaired_output)
    logger.info(f"[SUCCESS] [{request_id}] ‚úÖ Valida√ß√£o OK ap√≥s reparo")
    
    # Registra m√©trica de reparo
    record_repair_attempt("success")
```

#### Passo 4: Preenchimento da Idempotency Key

```python
# Preenche a chave de idempot√™ncia se poss√≠vel
idem_key = normalized.compute_idempotency_key()
if idem_key:
    analyzed.idempotency_key = idem_key
    logger.debug(f"[IDEM] [{request_id}] üîë Idempotency key: {idem_key[:16]}...")
else:
    # Se n√£o for poss√≠vel calcular, usa placeholder
    analyzed.idempotency_key = "no-idempotency-key-available"
    logger.warning(
        f"[IDEM] [{request_id}] ‚ö†Ô∏è N√£o foi poss√≠vel calcular idempotency_key"
    )

return analyzed
```

---

## üé≠ Valida√ß√£o de Sentimento

### Consist√™ncia Label ‚Üî Score

O `AnalyzedMeeting` possui um validador customizado que **garante consist√™ncia** entre `sentiment_label` e `sentiment_score`:

```python
@model_validator(mode='after')
def validate_sentiment_consistency(self):
    """
    Regras de consist√™ncia:
    - "positive": score >= 0.6
    - "neutral": 0.4 <= score < 0.6
    - "negative": score < 0.4
    """
    label = self.sentiment_label
    score = self.sentiment_score
    
    if label == "positive" and score < 0.6:
        raise ValueError(
            f"sentiment_label 'positive' requer score >= 0.6, recebido: {score}"
        )
    elif label == "neutral" and not (0.4 <= score < 0.6):
        raise ValueError(
            f"sentiment_label 'neutral' requer 0.4 <= score < 0.6, recebido: {score}"
        )
    elif label == "negative" and score >= 0.4:
        raise ValueError(
            f"sentiment_label 'negative' requer score < 0.4, recebido: {score}"
        )
    
    return self
```

**Exemplos:**

```python
# ‚úÖ V√ÅLIDO - Consistente
AnalyzedMeeting(
    sentiment_label="positive",
    sentiment_score=0.85,  # >= 0.6 ‚úì
    # ... outros campos
)

# ‚ùå INV√ÅLIDO - Inconsistente
AnalyzedMeeting(
    sentiment_label="positive",
    sentiment_score=0.3,  # < 0.6 ‚úó
    # ... outros campos
)
# ValidationError: sentiment_label 'positive' requer score >= 0.6, recebido: 0.3

# ‚úÖ V√ÅLIDO - Neutral
AnalyzedMeeting(
    sentiment_label="neutral",
    sentiment_score=0.5,  # 0.4 <= 0.5 < 0.6 ‚úì
    # ... outros campos
)

# ‚ùå INV√ÅLIDO - Neutral com score alto
AnalyzedMeeting(
    sentiment_label="neutral",
    sentiment_score=0.7,  # >= 0.6 ‚úó
    # ... outros campos
)
# ValidationError: sentiment_label 'neutral' requer 0.4 <= score < 0.6, recebido: 0.7
```

### Tabela de Consist√™ncia

| sentiment_label | sentiment_score | V√°lido? | Exemplo de Reuni√£o |
|-----------------|-----------------|---------|-------------------|
| **positive** | 0.85 | ‚úÖ | Cliente muito satisfeito, fechou neg√≥cio |
| **positive** | 0.6 | ‚úÖ | Cliente satisfeito, sem obje√ß√µes graves |
| **positive** | 0.55 | ‚ùå | Score muito baixo para "positive" |
| **neutral** | 0.5 | ‚úÖ | Cliente sem emo√ß√µes fortes |
| **neutral** | 0.45 | ‚úÖ | Cliente levemente positivo |
| **neutral** | 0.7 | ‚ùå | Score muito alto para "neutral" |
| **negative** | 0.3 | ‚úÖ | Cliente insatisfeito, com obje√ß√µes |
| **negative** | 0.1 | ‚úÖ | Cliente muito frustrado |
| **negative** | 0.5 | ‚ùå | Score muito alto para "negative" |

---

## üîÅ Resili√™ncia e Retry

### Decorator `@retry`

Id√™ntico ao Extractor, o Analyzer usa o mesmo sistema de retry:

```python
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type,
)

@retry(
    reraise=True,
    stop=stop_after_attempt(3),             # M√°ximo 3 tentativas
    wait=wait_exponential(                  # Backoff exponencial
        multiplier=0.5, 
        min=0.5, 
        max=5.0
    ),
    retry=retry_if_exception_type((         # Apenas para esses erros
        RateLimitError,                      # 429 - Too Many Requests
        APITimeoutError,                     # Timeout de rede
        APIError                             # Erro gen√©rico da API
    )),
    before_sleep=log_retry_attempt          # ‚Üê Log antes de cada retry
)
async def analyze_sentiment_chain(...):
    ...
```

### Log de Retry (via `log_retry_attempt`)

```python
# utils/retry_logger.py
def log_retry_attempt(retry_state):
    """Log antes de cada retry."""
    attempt = retry_state.attempt_number
    wait_time = retry_state.next_action.sleep
    exception = retry_state.outcome.exception()
    
    logger.warning(
        f"[RETRY] Tentativa {attempt}/3 falhou | "
        f"erro={type(exception).__name__} | "
        f"aguardando {wait_time:.1f}s antes de retry..."
    )
```

**Exemplo de Sequ√™ncia de Logs:**

```
[ANALYZE] [req-123] üß† Iniciando an√°lise de sentimento | transcript_len=790
[req-123] ü§ñ Chamada √† OpenAI | metadata_fields=2
[ERROR] [req-123] ‚ùå OpenAI API Error: RateLimitError - Too Many Requests
[RETRY] Tentativa 1/3 falhou | erro=RateLimitError | aguardando 0.5s...
[req-123] ü§ñ Chamada √† OpenAI | metadata_fields=2 (RETRY 2/3)
[RESPONSE] [req-123] ‚úÖ OpenAI API respondeu | duration=4.2s
[SUCCESS] [req-123] ‚úÖ Valida√ß√£o Pydantic OK
[SUCCESS] [req-123] üéâ An√°lise conclu√≠da | sentiment=positive | score=0.85
```

---

## üîß Sistema de Reparo

### Quando o Reparo √© Ativado?

Quando a **valida√ß√£o Pydantic falha** no output do LLM:

**Cen√°rio 1: Inconsist√™ncia label ‚Üî score**
```python
# LLM retornou
{
    "sentiment_label": "positive",
    "sentiment_score": 0.3,  # ‚ùå Inconsistente!
    ...
}

# Pydantic tenta validar
analyzed = AnalyzedMeeting.model_validate(raw_output)
# ValidationError: sentiment_label 'positive' requer score >= 0.6, recebido: 0.3

# Sistema detecta erro e chama reparo
```

**Cen√°rio 2: Summary muito curto**
```python
{
    "summary": "Breve resumo",  # ‚ùå Apenas 2 palavras (precisa 100-200)
    ...
}

# ValidationError: summary deve ter 100-200 palavras, tem 2
```

### Fun√ß√£o `repair_json()` (Compartilhada)

```python
# utils/json_repair.py
async def repair_json(
    malformed_output: dict,
    validation_error: str,
    normalized: NormalizedInput,
    request_id: str,
    schema_type: str  # ‚Üê "analyze" ou "extract"
) -> dict:
    """
    Tenta reparar um JSON malformado reenviando ao LLM com o erro.
    
    Args:
        schema_type: "analyze" ou "extract" (determina schema esperado)
    """
    
    logger.warning(
        f"[REPAIR] [{request_id}] üîß Tentando reparar JSON | "
        f"schema={schema_type} | erro={validation_error[:200]}"
    )
    
    # Carrega prompt de reparo do Hub
    repair_prompt_hub = os.getenv("JSON_REPAIRER_PROMPT_HUB_NAME")
    repair_prompt = hub.pull(repair_prompt_hub)
    
    # Seleciona schema correto
    if schema_type == "analyze":
        schema_doc = """
        - sentiment_label: "positive"|"neutral"|"negative"
        - sentiment_score: float (0.0-1.0)
        - Consist√™ncia: positive (‚â•0.6), neutral (0.4-0.6), negative (<0.4)
        - summary: string (100-200 palavras EXATAS)
        - key_points: array de strings
        - action_items: array de strings
        - risks: array de strings (pode ser vazio [])
        """
    else:
        schema_doc = """
        - summary: string (100-200 palavras EXATAS)
        - key_points: array de strings
        - action_items: array de strings
        - topics: array de strings
        """
    
    # Chain de reparo
    repair_chain = repair_prompt | llm | parser
    
    repaired = await repair_chain.ainvoke({
        "malformed_json": json.dumps(malformed_output, ensure_ascii=False, indent=2),
        "error": validation_error,
        "transcript_preview": sanitize_transcript_for_log(normalized.transcript, 500),
        "schema": schema_doc
    })
    
    logger.info(f"[REPAIR] [{request_id}] ‚úÖ JSON reparado com sucesso")
    return repaired
```

### Exemplo de Reparo - Analyzer

**Input para reparo:**
```json
{
  "malformed_json": {
    "sentiment_label": "positive",
    "sentiment_score": 0.3,
    "summary": "Cliente satisfeito... (150 palavras)",
    ...
  },
  "error": "sentiment_label 'positive' requer score >= 0.6, recebido: 0.3",
  "schema": "analyze"
}
```

**LLM processa e retorna JSON corrigido:**
```json
{
  "sentiment_label": "neutral",
  "sentiment_score": 0.45,
  "summary": "Cliente satisfeito... (150 palavras)",
  ...
}
```
**Valida√ß√£o (2¬™ tentativa):**
```python
analyzed = AnalyzedMeeting.model_validate(repaired_output)
# ‚úÖ OK! (neutral com score 0.45 √© consistente)
```

---

## üìä Logging e Seguran√ßa

### Estrutura de Logs

Todos os logs seguem o padr√£o com tags espec√≠ficas:

```
[TAG] [REQUEST_ID] emoji Mensagem | contexto_key=valor
```

**Tags usadas no Analyzer:**
- `[ANALYZE]` - In√≠cio da an√°lise
- `[RESPONSE]` - Resposta da OpenAI
- `[SUCCESS]` - An√°lise conclu√≠da
- `[ERROR]` - Erro na API
- `[VALIDATION]` - Erro de valida√ß√£o
- `[REPAIR]` - Reparo de JSON
- `[IDEM]` - Idempotency key

### Exemplo de Sequ√™ncia de Logs Completa

```
[ANALYZE] [req-abc] üß† Iniciando an√°lise de sentimento | transcript_len=790 | has_metadata=sim
[req-abc] ü§ñ Chamada √† OpenAI | metadata_fields=3 | transcript_preview=Cliente: Estou muito...
[RESPONSE] [req-abc] ‚úÖ OpenAI API respondeu com sucesso | duration=3.8s | output_keys=['sentiment_label', 'sentiment_score', ...]
[SUCCESS] [req-abc] ‚úÖ Valida√ß√£o Pydantic OK na primeira tentativa
[SUCCESS] [req-abc] üéâ An√°lise conclu√≠da com sucesso | sentiment=positive | score=0.85 | summary_words=169 | risks=0
[IDEM] [req-abc] üîë Idempotency key calculada: 7e3e97ffd83f...
```

### O que √â e N√ÉO √â Logado

‚úÖ **√â logado:**
- Request ID (rastreamento)
- Tamanho da transcri√ß√£o (n√∫mero de caracteres)
- Presen√ßa de metadados (sim/n√£o)
- N√∫mero de campos de metadata
- Preview de 100 chars da transcri√ß√£o
- Chaves do JSON retornado pelo LLM
- Status de valida√ß√£o (OK/FALHA)
- Sentiment label e score
- N√∫mero de palavras do summary
- Quantidade de key_points/action_items/risks
- Tipos de erros (sem detalhes sens√≠veis)
- Dura√ß√£o das chamadas

‚ùå **N√ÉO √© logado:**
- Transcri√ß√£o completa
- Nomes de clientes/bankers
- Conte√∫do do resumo
- IDs completos de clientes
- CPFs, emails, telefones
- Detalhes financeiros
- Insights espec√≠ficos

---

## üéì Exemplos Pr√°ticos

### Exemplo 1: An√°lise Positiva (1¬™ Tentativa)

```python
# Input
normalized = NormalizedInput(
    transcript="Cliente: Estou muito satisfeito com a proposta! Vamos fechar hoje mesmo.",
    meeting_id="MTG001",
    customer_id="CUST001",
    meet_date=datetime(2025, 9, 10, 14, 30)
)

# Chamada
analyzed = await analyze_sentiment_chain(normalized, "req-123")

# Logs:
# [ANALYZE] [req-123] üß† Iniciando an√°lise | transcript_len=68
# [req-123] ü§ñ Chamada √† OpenAI | metadata_fields=3
# [RESPONSE] [req-123] ‚úÖ OpenAI respondeu | duration=3.2s
# [SUCCESS] [req-123] ‚úÖ Valida√ß√£o OK
# [SUCCESS] [req-123] üéâ An√°lise conclu√≠da | sentiment=positive | score=0.92

# Output
print(analyzed.sentiment_label)  # "positive"
print(analyzed.sentiment_score)  # 0.92
print(len(analyzed.summary.split()))  # 145 palavras
print(analyzed.risks)  # []
```

### Exemplo 2: An√°lise Negativa com Riscos

```python
# Input
normalized = NormalizedInput(
    transcript="Cliente: Estou preocupado com as taxas. N√£o sei se consigo pagar...",
    meeting_id="MTG002",
    customer_id="CUST002",
    meet_date=datetime(2025, 9, 11, 10, 0)
)

# Chamada
analyzed = await analyze_sentiment_chain(normalized, "req-456")

# Output
print(analyzed.sentiment_label)  # "negative"
print(analyzed.sentiment_score)  # 0.25
print(analyzed.risks)
# [
#   "Cliente demonstrou preocupa√ß√£o com capacidade de pagamento",
#   "Obje√ß√£o forte sobre taxas cobradas"
# ]
```

### Exemplo 3: Reparo por Inconsist√™ncia

```python
# LLM retornou (inconsistente)
{
    "sentiment_label": "positive",
    "sentiment_score": 0.4,  # ‚ùå Deveria ser >= 0.6
    ...
}

# Logs:
# [VALIDATION] [req-789] ‚ö†Ô∏è Valida√ß√£o falhou | erro=ValueError: sentiment_label 'positive' requer score >= 0.6, recebido: 0.4
# [REPAIR] [req-789] üîß Tentando reparar JSON | schema=analyze
# [REPAIR] [req-789] ‚úÖ JSON reparado com sucesso
# [SUCCESS] [req-789] ‚úÖ Valida√ß√£o OK ap√≥s reparo

# JSON reparado
{
    "sentiment_label": "neutral",
    "sentiment_score": 0.45,  # ‚úÖ Consistente
    ...
}
```

### Exemplo 4: Retry por Rate Limit

```python
# Tentativa 1
[ANALYZE] [req-999] üß† Iniciando an√°lise
[ERROR] [req-999] ‚ùå OpenAI API Error: RateLimitError
[RETRY] Tentativa 1/3 falhou | erro=RateLimitError | aguardando 0.5s...

# Tentativa 2
[req-999] ü§ñ Chamada √† OpenAI (RETRY 2/3)
[RESPONSE] [req-999] ‚úÖ OpenAI respondeu | duration=4.1s
[SUCCESS] [req-999] ‚úÖ Valida√ß√£o OK
```

---

## üí° Dicas e Boas Pr√°ticas

### 1. Temperature Adequada por Uso

```python
# ‚úÖ BOM - Extrator (determin√≠stico)
llm_extract = ChatOpenAI(temperature=0)

# ‚úÖ BOM - Analyzer (levemente criativo)
llm_analyze = ChatOpenAI(temperature=0.2)

# ‚ùå RUIM - Analyzer muito criativo (inconsistente)
llm_analyze = ChatOpenAI(temperature=0.7)
```

### 2. Valida√ß√£o de Consist√™ncia

```python
# ‚úÖ BOM - Validador customizado
@model_validator(mode='after')
def validate_sentiment_consistency(self):
    # Garante label ‚Üî score consistentes
    ...

# ‚ùå RUIM - Sem valida√ß√£o (aceita inconsist√™ncias)
sentiment_label: Literal["positive", "neutral", "negative"]
sentiment_score: float  # Sem check de consist√™ncia
```

### 3. Logs Estruturados com Emojis

```python
# ‚úÖ BOM - F√°cil de identificar visualmente
logger.info(f"[ANALYZE] [{request_id}] üß† Iniciando an√°lise")
logger.info(f"[SUCCESS] [{request_id}] üéâ An√°lise conclu√≠da")
logger.error(f"[ERROR] [{request_id}] ‚ùå Falha na API")

# ‚ùå RUIM - Dif√≠cil de parsear
logger.info(f"Analisando sentimento para {request_id}")
```

### 4. Campo `risks` Opcional mas Sempre Lista

```python
# ‚úÖ BOM - Sempre retorna lista (pode ser vazia)
risks: List[str] = Field(default_factory=list)

# ‚ùå RUIM - None pode causar erros
risks: Optional[List[str]] = None
```

---

## üìä M√©tricas de Performance

| M√©trica | Valor T√≠pico | Observa√ß√£o |
|---------|--------------|------------|
| **Tempo m√©dio de an√°lise** | 3-6 segundos | Temperature 0.2 √© ligeiramente mais lento |
| **Taxa de sucesso (1¬™ tentativa)** | ~92% | Menor que Extractor devido a valida√ß√µes complexas |
| **Taxa de reparo bem-sucedido** | ~80% | Inconsist√™ncia label ‚Üî score √© comum |
| **Taxa de retry por rate limit** | ~2% | Id√™ntico ao Extractor |
| **Taxa de falha definitiva** | <1% | Muito raro |
| **% Positive** | ~45% | Depende do dataset |
| **% Neutral** | ~35% | Depende do dataset |
| **% Negative** | ~20% | Depende do dataset |

---

## üîç Debugging

### Como debugar erros de an√°lise?

1. **Verifique os logs com o Request ID:**
```bash
grep "req-123" logs.txt | grep ANALYZE
```

2. **Teste o prompt manualmente:**
```python
# No console Python
from app.analyzers.analyzer import prompt, llm, parser

result = await (prompt | llm).ainvoke({
    "transcript": "Cliente: Teste...",
    "metadata_json": "{}"
})
print(result.content)
```

3. **Valide o output manualmente:**
```python
from app.models.schemas_analyze import AnalyzedMeeting

try:
    analyzed = AnalyzedMeeting.model_validate(result)
except Exception as e:
    print(f"Erro: {e}")
```

4. **Teste consist√™ncia label ‚Üî score:**
```python
# Deve falhar
AnalyzedMeeting(
    sentiment_label="positive",
    sentiment_score=0.3,  # ‚ùå
    ...
)
# ValidationError: sentiment_label 'positive' requer score >= 0.6, recebido: 0.3
```

---

## üìö Diferen√ßas Extractor vs Analyzer

| Aspecto | Extractor | Analyzer |
|---------|-----------|----------|
| **Temperature** | 0 (determin√≠stico) | 0.2 (levemente criativo) |
| **Output Principal** | Dados estruturados | Sentimento + insights |
| **Valida√ß√µes** | Summary 100-200 palavras | + Consist√™ncia label ‚Üî score |
| **Campo Diferencial** | `topics` | `sentiment_label`, `sentiment_score`, `risks` |
| **Prompt** | Definido em c√≥digo | Carregado do LangChain Hub |
| **Uso** | Extra√ß√£o de fatos | An√°lise subjetiva |
| **Taxa de Reparo** | ~85% | ~80% (valida√ß√µes mais complexas) |

---

## üìö Refer√™ncias

- **LangChain Docs:** https://python.langchain.com/docs/get_started/introduction
- **LangChain Hub:** https://smith.langchain.com/hub
- **OpenAI API:** https://platform.openai.com/docs/api-reference
- **Tenacity (Retry):** https://tenacity.readthedocs.io/

---

**Pr√≥ximo:** [05-TESTS.md](05-TESTS.md) - Como testar o Analyzer


