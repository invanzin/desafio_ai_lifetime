"""
Cliente centralizado para intera√ß√µes com OpenAI via LangChain.

Este m√≥dulo fornece uma abstra√ß√£o reutiliz√°vel para configurar e usar
a API da OpenAI em diferentes partes do projeto. Centraliza configura√ß√µes,
valida√ß√µes e facilita manuten√ß√£o e testes.

Uso:
    >>> from llm.openai_client import OpenAIClient
    >>> client = OpenAIClient()
    >>> llm = client.get_llm()
    >>> # Use o LLM em suas chains do LangChain
"""

import os
from typing import Optional
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI

# Carrega vari√°veis de ambiente
load_dotenv()


class OpenAIClient:
    """
    Wrapper centralizado para gerenciar a configura√ß√£o e inst√¢ncia do LLM da OpenAI.
    
    Esta classe encapsula toda a l√≥gica de configura√ß√£o da OpenAI API,
    incluindo valida√ß√£o de credenciais, sele√ß√£o de modelo e par√¢metros.
    Facilita reutiliza√ß√£o em diferentes agentes (extractor, analyzer, etc).
    
    Attributes:
        api_key (str): Chave de API da OpenAI.
        default_model (str): Modelo padr√£o (ex: gpt-4o, gpt-4-turbo).
        default_temperature (float): Temperature padr√£o para gera√ß√£o.
        default_timeout (float): Timeout padr√£o em segundos.
    
    Example:
        >>> # Uso b√°sico
        >>> client = OpenAIClient()
        >>> llm = client.get_llm()
        
        >>> # Com configura√ß√µes customizadas
        >>> llm_criativo = client.get_llm(temperature=0.7)
        >>> llm_rapido = client.get_llm(model="gpt-3.5-turbo")
    """
    
    def __init__(
        self,
        api_key: Optional[str] = None,
        default_model: Optional[str] = None,
        default_temperature: float = 0.0,
        default_timeout: float = 30.0
    ):
        """
        Inicializa o cliente OpenAI com configura√ß√µes padr√£o.
        
        Args:
            api_key (Optional[str]): Chave de API. Se None, l√™ de OPENAI_API_KEY env var.
            default_model (Optional[str]): Modelo padr√£o. Se None, l√™ de OPENAI_MODEL env var.
            default_temperature (float): Temperature padr√£o (0.0 = determin√≠stico).
            default_timeout (float): Timeout padr√£o em segundos.
        
        Raises:
            ValueError: Se OPENAI_API_KEY n√£o estiver configurada.
        """
        # Obt√©m API key (prioridade: par√¢metro > env var)
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError(
                "OPENAI_API_KEY n√£o configurada. "
                "Configure a vari√°vel de ambiente ou passe como par√¢metro."
            )
        
        # Obt√©m modelo padr√£o (prioridade: par√¢metro > env var > fallback)
        self.default_model = default_model or os.getenv("OPENAI_MODEL", "gpt-4o")
        
        # Armazena configura√ß√µes padr√£o
        self.default_temperature = default_temperature
        self.default_timeout = default_timeout
        
        # Cache da inst√¢ncia LLM padr√£o (criada sob demanda)
        self._default_llm: Optional[ChatOpenAI] = None
    
    def get_llm(
        self,
        model: Optional[str] = None,
        temperature: Optional[float] = None,
        timeout: Optional[float] = None,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> ChatOpenAI:
        """
        Retorna uma inst√¢ncia configurada do ChatOpenAI.
        
        Este m√©todo cria ou reutiliza uma inst√¢ncia do LLM com as configura√ß√µes
        especificadas. Se todos os par√¢metros forem None, retorna uma inst√¢ncia
        cacheada com configura√ß√µes padr√£o (para melhor performance).
        
        Args:
            model (Optional[str]): Modelo a usar. Se None, usa default_model.
            temperature (Optional[float]): Temperature (0.0-2.0). Se None, usa default.
            timeout (Optional[float]): Timeout em segundos. Se None, usa default.
            max_tokens (Optional[int]): M√°ximo de tokens na resposta. Se None, sem limite.
            **kwargs: Outros par√¢metros do ChatOpenAI.
        
        Returns:
            ChatOpenAI: Inst√¢ncia configurada do modelo de linguagem.
        
        Example:
            >>> client = OpenAIClient()
            
            >>> # LLM padr√£o (determin√≠stico, gpt-4o)
            >>> llm = client.get_llm()
            
            >>> # LLM para an√°lise criativa
            >>> llm_criativo = client.get_llm(temperature=0.7)
            
            >>> # LLM r√°pido e barato
            >>> llm_rapido = client.get_llm(model="gpt-3.5-turbo", timeout=10.0)
            
            >>> # LLM com limite de tokens (para controle de custo)
            >>> llm_controlado = client.get_llm(max_tokens=500)
        """
        # Se todos os par√¢metros opcionais s√£o None, reutiliza inst√¢ncia cacheada
        if all(p is None for p in [model, temperature, timeout, max_tokens]) and not kwargs:
            if self._default_llm is None:
                self._default_llm = self._create_llm()
            return self._default_llm
        
        # Cria nova inst√¢ncia com configura√ß√µes customizadas
        return self._create_llm(
            model=model,
            temperature=temperature,
            timeout=timeout,
            max_tokens=max_tokens,
            **kwargs
        )
    
    def _create_llm(
        self,
        model: Optional[str] = None,
        temperature: Optional[float] = None,
        timeout: Optional[float] = None,
        max_tokens: Optional[int] = None,
        **kwargs
        ) -> ChatOpenAI:
        """
        Cria uma nova inst√¢ncia do ChatOpenAI (m√©todo interno).
        
        Args:
            model (Optional[str]): Modelo a usar.
            temperature (Optional[float]): Temperature de gera√ß√£o.
            timeout (Optional[float]): Timeout em segundos.
            max_tokens (Optional[int]): M√°ximo de tokens na resposta.
            **kwargs: Outros par√¢metros do ChatOpenAI.
        
        Returns:
            ChatOpenAI: Nova inst√¢ncia configurada.
        """
        # Usa valores passados ou fallback para defaults
        final_model = model if model is not None else self.default_model
        final_temperature = temperature if temperature is not None else self.default_temperature
        final_timeout = timeout if timeout is not None else self.default_timeout
        
        # Prepara kwargs para ChatOpenAI
        llm_kwargs = {
            "model": final_model,
            "temperature": final_temperature,
            "timeout": final_timeout,
            "api_key": self.api_key,
            # üî• HABILITANDO COLETA DE TOKENS:
            "model_kwargs": {
                "stream_usage": True,  # Para capturar usage em streaming
            },
            **kwargs
        }
        
        # Adiciona max_tokens apenas se especificado
        if max_tokens is not None:
            llm_kwargs["max_tokens"] = max_tokens
        
        return ChatOpenAI(**llm_kwargs)
    
    def get_model_info(self) -> dict:
        """
        Retorna informa√ß√µes sobre a configura√ß√£o atual.
        
        √ötil para debugging e logging de configura√ß√£o.
        
        Returns:
            dict: Dicion√°rio com informa√ß√µes de configura√ß√£o.
        
        Example:
            >>> client = OpenAIClient()
            >>> print(client.get_model_info())
            {
                'default_model': 'gpt-4o',
                'default_temperature': 0.0,
                'default_timeout': 30.0,
                'api_key_configured': True
            }
        """
        return {
            "default_model": self.default_model,
            "default_temperature": self.default_temperature,
            "default_timeout": self.default_timeout,
            "api_key_configured": bool(self.api_key),
            "api_key_prefix": self.api_key[:10] + "..." if self.api_key else None
        }


# ============================================================================
# INST√ÇNCIA GLOBAL PADR√ÉO (para conveni√™ncia)
# ============================================================================

# Inst√¢ncia global singleton para uso simples
# Voc√™ pode importar diretamente: from llm.openai_client import default_client
try:
    default_client = OpenAIClient()
except ValueError:
    # Se n√£o houver API key, default_client ser√° None
    default_client = None

